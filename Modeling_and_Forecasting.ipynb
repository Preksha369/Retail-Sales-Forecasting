{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Modeling and Forecasting Notebook\n",
        "\n",
        "This notebook is a continuation of the previous **EDA (Exploratory Data Analysis) notebook**. Having explored and cleaned the data in the EDA phase, this notebook focuses on:\n",
        "\n",
        "- **Preparing the data** for modeling (including filling missing dates to ensure continuous weekly data)\n",
        "- **Applying feature engineering** techniques such as lag features, rolling averages, and capping to improve predictive power\n",
        "- **Training XGBoost models** for each `SerialNum`\n",
        "- **Evaluating model performance** using accuracy metrics on a 3-month hold-out validation set\n",
        "- **Forecasting weekly sales** for the next 3 months (September, October, and November 2024) as per the assignment requirement\n",
        "\n",
        "Feature engineering steps are modular, allowing toggling on/off of:\n",
        "- Lag features (with adjustable lag window)\n",
        "- Rolling means (with configurable window sizes)\n",
        "- Capping/extreme value control\n",
        "\n",
        "This design allows us to experiment and evaluate the impact of each technique on model accuracy.\n",
        "\n",
        "The final output of this notebook includes:\n",
        " - Monthly and overall validation accuracy  \n",
        " - Forecasted weekly sales for Sep–Nov 2024  \n",
        "\n",
        "All steps are designed to ensure **consistent and explainable demand forecasting**, aligned with business requirements.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "XF8PEAH4ReQ3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Firstly, we import the essential libraries required for the analysis and modeling:"
      ],
      "metadata": {
        "id": "yGvVCefcDvJk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import xgboost as xgb\n",
        "from sklearn.model_selection import train_test_split\n",
        "import joblib\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "jFfB_9Y393_b"
      },
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, we first load our cleaned dataset from the CSV file into a pandas DataFrame so that we can begin analysis and modeling. We then convert the weekend_date column into a proper datetime format, explicitly specifying dayfirst=True because our data uses day-first dates. After ensuring that our dates are in datetime format, we sort the data by SerailNum and weekend_date to maintain the correct chronological order for each time series, which is essential for creating lag features and ensuring the model respects time dependencies. Finally, we reset the index to keep the DataFrame tidy and print a quick preview to confirm the data has loaded and sorted correctly."
      ],
      "metadata": {
        "id": "EvbmKALKD5An"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load data\n",
        "data_path = 'cleaned_Assessment.csv'\n",
        "df = pd.read_csv(data_path)\n",
        "df['weekend_date'] = pd.to_datetime(df['weekend_date'], dayfirst=True)\n",
        "\n",
        "# Sort for time order\n",
        "df = df.sort_values(['SerailNum', 'weekend_date']).reset_index(drop=True)\n",
        "print(df.head())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ruu1K6xO9-8W",
        "outputId": "25f2b9ff-cecf-4f45-96b8-ec1cd48c40c3"
      },
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  weekend_date   channel brand category sub_category  SerailNum  quantity\n",
            "0   2022-06-04  Channel1    B1     Cat2     Sub-Cat2          1        56\n",
            "1   2022-06-11  Channel1    B1     Cat2     Sub-Cat2          1       122\n",
            "2   2022-06-18  Channel1    B1     Cat2     Sub-Cat2          1       102\n",
            "3   2022-06-25  Channel1    B1     Cat2     Sub-Cat2          1       128\n",
            "4   2022-07-02  Channel1    B1     Cat2     Sub-Cat2          1        97\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, we ensure that our time series data has no missing weekly intervals for each unique SerailNum. Forecasting models typically expect a complete, regular time grid. To achieve this, we first extract the unique SerailNum values and process them one at a time. For each serial, we determine its start and end dates and generate a continuous sequence of Saturdays between them using pd.date_range(). We then merge this full calendar with the existing data so that any missing weeks become visible as rows with nulls. To preserve metadata consistency (such as channel, brand, category, and sub_category), we forward-fill these columns—assuming these attributes remain constant over time. Finally, any missing quantity values (which represent missing sales for those weeks) are filled with zeros, indicating no sales in those weeks. We repeat this for all serials, concatenate them into a single DataFrame, and print the head to verify that our time grid is complete and the data is clean."
      ],
      "metadata": {
        "id": "WcRTpkCBETE-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Fill missing weekly dates\n",
        "filled_dfs = []\n",
        "serials = df['SerailNum'].unique()\n",
        "\n",
        "for serial in serials:\n",
        "    df_serial = df[df['SerailNum'] == serial].copy()\n",
        "    start_date = df_serial['weekend_date'].min()\n",
        "    end_date = df_serial['weekend_date'].max()\n",
        "    full_dates = pd.DataFrame({\n",
        "        'weekend_date': pd.date_range(start=start_date, end=end_date, freq='W-SAT')\n",
        "    })\n",
        "    merged = full_dates.merge(df_serial, on='weekend_date', how='left')\n",
        "    for col in ['channel', 'brand', 'category', 'sub_category', 'SerailNum']:\n",
        "        merged[col] = merged[col].fillna(method='ffill')\n",
        "    merged['quantity'] = merged['quantity'].fillna(0)\n",
        "    filled_dfs.append(merged)\n",
        "\n",
        "df_filled = pd.concat(filled_dfs).reset_index(drop=True)\n",
        "print(df_filled.head())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ipY4cEKq-AXY",
        "outputId": "02a21ca4-8ec5-455d-b479-9af8cd3ab785"
      },
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  weekend_date   channel brand category sub_category  SerailNum  quantity\n",
            "0   2022-06-04  Channel1    B1     Cat2     Sub-Cat2        1.0      56.0\n",
            "1   2022-06-11  Channel1    B1     Cat2     Sub-Cat2        1.0     122.0\n",
            "2   2022-06-18  Channel1    B1     Cat2     Sub-Cat2        1.0     102.0\n",
            "3   2022-06-25  Channel1    B1     Cat2     Sub-Cat2        1.0     128.0\n",
            "4   2022-07-02  Channel1    B1     Cat2     Sub-Cat2        1.0      97.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, we define switches to control feature engineering steps.\n",
        "\n",
        "- USE_LAG and LAG_N set whether to add lag features and how many weeks.\n",
        "\n",
        "- USE_ROLLING and ROLLING_WINDOWS set rolling average features.\n",
        "\n",
        "- USE_CAPPING and CAPPING_VALUE control outlier capping.\n",
        "\n",
        "By changing these flags, we can easily test different modeling strategies without rewriting code."
      ],
      "metadata": {
        "id": "BeYyQOKOFaek"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Feature Engineering Switches\n",
        "USE_LAG = True\n",
        "LAG_N = 4\n",
        "\n",
        "USE_ROLLING = True\n",
        "ROLLING_WINDOWS = [3, 6]\n",
        "\n",
        "USE_CAPPING = True\n",
        "CAPPING_VALUE = 0.99 # e.g., 99th percentile\n",
        "\n",
        "print(f\"Switches - USE_LAG={USE_LAG}, USE_ROLLING={USE_ROLLING}, USE_CAPPING={USE_CAPPING}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jfO72yCz-FKK",
        "outputId": "2f96c639-8474-4cf8-9586-598b2f08261b"
      },
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Switches - USE_LAG=True, USE_ROLLING=True, USE_CAPPING=True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We now engineer features to improve model learning:\n",
        "\n",
        "- Lag Features: Add past values of quantity to capture temporal dependencies.\n",
        "\n",
        "- Rolling Features: Compute smoothed averages over time windows.\n",
        "\n",
        "- Capping: Limit extreme values to reduce outlier impact.\n",
        "\n",
        "Each transformation is controlled by switches so you can easily turn them on or off. The result is a flexible design to experiment with different feature combinations."
      ],
      "metadata": {
        "id": "o1YYpAMaGKOe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Feature Engineering\n",
        "df_features = df_filled.copy()\n",
        "df_features = df_features.sort_values(['SerailNum', 'weekend_date']).reset_index(drop=True)\n",
        "\n",
        "if USE_LAG:\n",
        "    for lag in range(1, LAG_N + 1):\n",
        "        df_features[f'quantity_lag_{lag}'] = df_features.groupby('SerailNum')['quantity'].shift(lag)\n",
        "\n",
        "if USE_ROLLING:\n",
        "    for window in ROLLING_WINDOWS:\n",
        "        df_features[f'quantity_roll{window}'] = df_features.groupby('SerailNum')['quantity'].transform(lambda x: x.rolling(window).mean())\n",
        "\n",
        "if USE_CAPPING:\n",
        "    cap_value = df_features['quantity'].quantile(CAPPING_VALUE) if CAPPING_VALUE else df_features['quantity'].max()\n",
        "    df_features['quantity_capped'] = df_features['quantity'].clip(upper=cap_value)\n",
        "    target_col = 'quantity_capped'\n",
        "else:\n",
        "    target_col = 'quantity'\n",
        "\n",
        "print(df_features.head(10))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0QFsyklS-I1k",
        "outputId": "63a4996a-54c2-4ea9-d286-6742dbeacd93"
      },
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  weekend_date   channel brand category sub_category  SerailNum  quantity  \\\n",
            "0   2022-06-04  Channel1    B1     Cat2     Sub-Cat2        1.0      56.0   \n",
            "1   2022-06-11  Channel1    B1     Cat2     Sub-Cat2        1.0     122.0   \n",
            "2   2022-06-18  Channel1    B1     Cat2     Sub-Cat2        1.0     102.0   \n",
            "3   2022-06-25  Channel1    B1     Cat2     Sub-Cat2        1.0     128.0   \n",
            "4   2022-07-02  Channel1    B1     Cat2     Sub-Cat2        1.0      97.0   \n",
            "5   2022-07-09  Channel1    B1     Cat2     Sub-Cat2        1.0     114.0   \n",
            "6   2022-07-16  Channel1    B1     Cat2     Sub-Cat2        1.0      84.0   \n",
            "7   2022-07-23  Channel1    B1     Cat2     Sub-Cat2        1.0     102.0   \n",
            "8   2022-07-30  Channel1    B1     Cat2     Sub-Cat2        1.0     115.0   \n",
            "9   2022-08-06  Channel1    B1     Cat2     Sub-Cat2        1.0      91.0   \n",
            "\n",
            "   quantity_lag_1  quantity_lag_2  quantity_lag_3  quantity_lag_4  \\\n",
            "0             NaN             NaN             NaN             NaN   \n",
            "1            56.0             NaN             NaN             NaN   \n",
            "2           122.0            56.0             NaN             NaN   \n",
            "3           102.0           122.0            56.0             NaN   \n",
            "4           128.0           102.0           122.0            56.0   \n",
            "5            97.0           128.0           102.0           122.0   \n",
            "6           114.0            97.0           128.0           102.0   \n",
            "7            84.0           114.0            97.0           128.0   \n",
            "8           102.0            84.0           114.0            97.0   \n",
            "9           115.0           102.0            84.0           114.0   \n",
            "\n",
            "   quantity_roll3  quantity_roll6  quantity_capped  \n",
            "0             NaN             NaN             56.0  \n",
            "1             NaN             NaN            122.0  \n",
            "2       93.333333             NaN            102.0  \n",
            "3      117.333333             NaN            128.0  \n",
            "4      109.000000             NaN             97.0  \n",
            "5      113.000000      103.166667            114.0  \n",
            "6       98.333333      107.833333             84.0  \n",
            "7      100.000000      104.500000            102.0  \n",
            "8      100.333333      106.666667            115.0  \n",
            "9      102.666667      100.500000             91.0  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next task i to split the data into training (older dates) and validation (last 3 months) sets. This simulates real forecasting by ensuring the model is trained on past data and tested on future periods without data leakage."
      ],
      "metadata": {
        "id": "FBTqsWWzHQFS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Train/Validation Split\n",
        "train_cutoff_date = df_features['weekend_date'].max() - pd.DateOffset(months=3)\n",
        "train_df = df_features[df_features['weekend_date'] <= train_cutoff_date]\n",
        "val_df = df_features[df_features['weekend_date'] > train_cutoff_date]\n",
        "\n",
        "print(f\"Training up to: {train_cutoff_date}\")\n",
        "print(f\"Train rows: {len(train_df)}, Val rows: {len(val_df)}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-u3oC9Ay-Mf3",
        "outputId": "0a77b0ad-7b7c-4cd4-b430-2f1217a50768"
      },
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training up to: 2024-05-31 00:00:00\n",
            "Train rows: 528, Val rows: 75\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, we train a separate XGBoost regression model for each SerialNum using the engineered features—lags and rolling averages. For each SerialNum, we filter its training data, drop rows with missing feature values (which can occur due to lags/rolling), and train a model to predict the quantity. The trained models are saved using joblib for later use in validation or forecasting. This approach allows each product to be modeled individually, capturing its unique trends and patterns.\n",
        "\n"
      ],
      "metadata": {
        "id": "-gT6GqT4H4J8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Train XGBoost Models by SerialNum\n",
        "feature_cols = [col for col in df_features.columns if 'lag' in col or 'roll' in col]\n",
        "print(f\"Using features: {feature_cols}\")\n",
        "\n",
        "serials = df_features['SerailNum'].unique()\n",
        "\n",
        "for serial in serials:\n",
        "    train_serial = train_df[train_df['SerailNum'] == serial].dropna(subset=feature_cols)\n",
        "    if train_serial.empty:\n",
        "        print(f\" No training rows for SerialNum {serial}\")\n",
        "        continue\n",
        "    X_train = train_serial[feature_cols]\n",
        "    y_train = train_serial[target_col]\n",
        "    model = xgb.XGBRegressor(n_estimators=100, random_state=42)\n",
        "    model.fit(X_train, y_train)\n",
        "    joblib.dump(model, f\"xgb_model_serial_{int(serial)}.joblib\")\n",
        "    print(f\"Trained model for SerialNum {serial}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CYNpiWb8Ilng",
        "outputId": "b6029458-9fda-4665-e6b4-359ed0319b6c"
      },
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using features: ['quantity_lag_1', 'quantity_lag_2', 'quantity_lag_3', 'quantity_lag_4', 'quantity_roll3', 'quantity_roll6']\n",
            "Trained model for SerialNum 1.0\n",
            "Trained model for SerialNum 2.0\n",
            "Trained model for SerialNum 3.0\n",
            "Trained model for SerialNum 4.0\n",
            "Trained model for SerialNum 5.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, we evaluate our trained models on the validation set. For each SerialNum, we load its trained XGBoost model and use it to predict the quantity on its validation data. We ensure that only rows with valid (non-NaN) feature values are used. The predictions (y_hat) are stored alongside actual values to compute errors. We also extract the month from the date to enable later monthly aggregation. Finally, we calculate the absolute error between predicted and actual quantity, preparing the data for accuracy analysis."
      ],
      "metadata": {
        "id": "AcLvQS52JQeD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Validation Predictions\n",
        "all_val_preds = []\n",
        "\n",
        "for serial in serials:\n",
        "    model_path = f\"xgb_model_serial_{int(serial)}.joblib\"\n",
        "    try:\n",
        "        model = joblib.load(model_path)\n",
        "    except:\n",
        "        continue\n",
        "\n",
        "    val_serial = val_df[val_df['SerailNum'] == serial].copy()\n",
        "    val_serial = val_serial.dropna(subset=feature_cols)\n",
        "    if val_serial.empty:\n",
        "        continue\n",
        "\n",
        "    X_val = val_serial[feature_cols]\n",
        "    val_serial['y_hat'] = model.predict(X_val)\n",
        "    all_val_preds.append(val_serial)\n",
        "\n",
        "val_predictions = pd.concat(all_val_preds).reset_index(drop=True)\n",
        "val_predictions['month'] = val_predictions['weekend_date'].dt.month\n",
        "val_predictions['error'] = abs(val_predictions['y_hat'] - val_predictions[target_col])\n"
      ],
      "metadata": {
        "id": "6RJl-9p7-svL"
      },
      "execution_count": 78,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Toevaluate model performance using intuitive accuracy metrics:\n",
        "\n",
        "- First, we group predictions by SerialNum and month to see how well each model performs for each product/series over time. We sum the absolute error and actual quantity to get an aggregated monthly accuracy per SerialNum.\n",
        "\n",
        "- Then, we compute an overall accuracy across all data by comparing total prediction error to total actual quantity.\n",
        "\n",
        "- Finally, we calculate accuracy per month across all products to identify seasonal or time-based variation in performance.\n",
        "\n",
        "This helps us understand and report model accuracy both overall and in detail, as required in the assignment.\n",
        "\n"
      ],
      "metadata": {
        "id": "tOFHhwGdJh0N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Accuracy by SerialNum and Month\n",
        "monthly_results = val_predictions.groupby(['SerailNum', 'month']).agg({\n",
        "    'error': 'sum',\n",
        "    target_col: 'sum'\n",
        "}).reset_index()\n",
        "monthly_results['accuracy'] = 1 - (monthly_results['error'] / monthly_results[target_col])\n",
        "print(\" Monthly Accuracy by SerialNum\")\n",
        "print(monthly_results)\n",
        "\n",
        "# Overall Accuracy\n",
        "overall_error = val_predictions['error'].sum()\n",
        "overall_quantity = val_predictions[target_col].sum()\n",
        "overall_accuracy = 1 - (overall_error / overall_quantity)\n",
        "print(f\"Overall Accuracy: {overall_accuracy:.4f}\")\n",
        "\n",
        "# Monthly Accuracy\n",
        "month_level = val_predictions.groupby('month').agg({\n",
        "    'error': 'sum',\n",
        "    target_col: 'sum'\n",
        "}).reset_index()\n",
        "month_level['accuracy'] = 1 - (month_level['error'] / month_level[target_col])\n",
        "print(\"Monthly Accuracy Overall\")\n",
        "print(month_level)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OOpcJF-M-wRd",
        "outputId": "d1496232-0bfc-4332-b792-b1adc1e17ca2"
      },
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Monthly Accuracy by SerialNum\n",
            "    SerailNum  month        error  quantity_capped   accuracy\n",
            "0         1.0      6    53.106384           148.00   0.641173\n",
            "1         1.0      7    30.926054           165.00   0.812569\n",
            "2         1.0      8    54.512875           297.00   0.816455\n",
            "3         2.0      6   655.100006          3488.00   0.812185\n",
            "4         2.0      7   134.122696           742.00   0.819242\n",
            "5         2.0      8   123.327438           903.00   0.863425\n",
            "6         3.0      6   794.288002          1269.00   0.374084\n",
            "7         3.0      7   426.382507           170.00  -1.508132\n",
            "8         3.0      8   505.143089           921.00   0.451528\n",
            "9         4.0      6  6317.679749         16011.00   0.605416\n",
            "10        4.0      7  4793.930144         14485.72   0.669058\n",
            "11        4.0      8  1444.043274          8235.00   0.824646\n",
            "12        5.0      6  1192.965431            15.00 -78.531029\n",
            "Overall Accuracy: 0.6473\n",
            "Monthly Accuracy Overall\n",
            "   month        error  quantity_capped  accuracy\n",
            "0      6  9013.139572         20931.00  0.569388\n",
            "1      7  5385.361401         15562.72  0.653958\n",
            "2      8  2127.026676         10356.00  0.794609\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Interpretation of Results\n",
        "\n",
        "After running the model with the chosen feature engineering settings (lags, rolling averages, capping), we evaluated its performance on the validation set:\n",
        "\n",
        "Monthly Accuracy by SerialNum: Shows how well the model predicts for each product (SerialNum) in each month. We see good accuracy for many products (often > 0.8), though some SerialNums have poor or even negative accuracy, suggesting they may need further investigation or may be harder to predict.\n",
        "\n",
        "Overall Accuracy: ~64.7%, indicating that in total, the model explains a good portion of the quantity variation over the entire validation period.\n",
        "\n",
        "Monthly Accuracy Overall: Breaks down accuracy across all products for June, July, and August. Accuracy improves over the months (from ~56.9% in June to ~79.5% in August), possibly reflecting seasonality or data distribution shifts.\n",
        "\n",
        "These results help us understand where the model is strong and where it needs improvement, supporting decisions about further tuning or feature engineering."
      ],
      "metadata": {
        "id": "T5epIgajKH4V"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Forecasting Sales for Sep–Nov\n",
        "Finally, I generate predictions for all Saturdays in September, October, and November 2024 (as required in the question). I used the trained XGBoost models for each SerialNum to forecast sales for these 13 weeks.\n",
        "\n",
        "The forecasting approach simulates each future week step by step. For each week:\n",
        "\n",
        "Lag Features: Past predicted values are used to compute lag inputs.\n",
        "\n",
        "Rolling Means: Rolling averages over prior predicted quantities are updated recursively.\n",
        "\n",
        "Capping: The same target transformation is used as in training.\n",
        "\n",
        "This recursive simulation ensures consistency with the training features, allowing the model to project future demand realistically. The final output is a table of forecasted quantities for each SerialNum and each Saturday in the target months.\n",
        "\n"
      ],
      "metadata": {
        "id": "Ah8xFM4VLEj1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Forecasting for Sep-Nov\n",
        "import pandas as pd\n",
        "\n",
        "# Define forecast range specifically for Sep, Oct, Nov\n",
        "forecast_start = pd.Timestamp('2024-09-01')\n",
        "forecast_end = pd.Timestamp('2024-11-30')\n",
        "\n",
        "# Generate all Saturdays in this range\n",
        "forecast_dates = pd.date_range(\n",
        "    start=forecast_start,\n",
        "    end=forecast_end,\n",
        "    freq='W-SAT'\n",
        ")\n",
        "print(\"Forecast Weeks:\", forecast_dates)\n",
        "\n",
        "all_forecasts = []\n",
        "\n",
        "for serial in serials:\n",
        "    print(f\"\\nGenerating forecast for SerialNum {serial}\")\n",
        "\n",
        "    # Start with this serial's existing data\n",
        "    serial_history = df_features[df_features['SerailNum'] == serial].copy()\n",
        "    serial_history = serial_history.sort_values('weekend_date')\n",
        "\n",
        "    # Copy to simulate appending forecasts\n",
        "    forecast_df = serial_history.copy()\n",
        "\n",
        "    for date in forecast_dates:\n",
        "        new_row = {\n",
        "            'SerailNum': serial,\n",
        "            'weekend_date': date\n",
        "        }\n",
        "\n",
        "        # Lag features\n",
        "        if USE_LAG:\n",
        "            for lag in range(1, LAG_N + 1):\n",
        "                if len(forecast_df) >= lag:\n",
        "                    new_row[f'quantity_lag_{lag}'] = forecast_df.iloc[-lag][target_col]\n",
        "                else:\n",
        "                    new_row[f'quantity_lag_{lag}'] = np.nan\n",
        "\n",
        "        # Rolling features\n",
        "        if USE_ROLLING:\n",
        "            for window in ROLLING_WINDOWS:\n",
        "                if len(forecast_df) >= window:\n",
        "                    new_row[f'quantity_roll{window}'] = forecast_df[target_col].iloc[-window:].mean()\n",
        "                else:\n",
        "                    new_row[f'quantity_roll{window}'] = np.nan\n",
        "\n",
        "        # Drop NA if lags/rollings insufficient\n",
        "        feature_input = pd.DataFrame([new_row])\n",
        "        feature_input = feature_input.dropna(subset=feature_cols)\n",
        "        if feature_input.empty:\n",
        "            print(f\" Skipping forecast for {serial} on {date.date()} due to insufficient lags/rollings\")\n",
        "            continue\n",
        "\n",
        "        # Load model\n",
        "        try:\n",
        "            model = joblib.load(f\"xgb_model_serial_{int(serial)}.joblib\")\n",
        "        except:\n",
        "            print(f\" Model for SerialNum {serial} not found\")\n",
        "            continue\n",
        "\n",
        "        # Predict\n",
        "        new_row[target_col] = model.predict(feature_input[feature_cols])[0]\n",
        "        forecast_df = pd.concat([forecast_df, pd.DataFrame([new_row])], ignore_index=True)\n",
        "\n",
        "        all_forecasts.append({\n",
        "            'SerailNum': serial,\n",
        "            'weekend_date': date,\n",
        "            'forecast_quantity': new_row[target_col]\n",
        "        })\n",
        "\n",
        "forecast_result = pd.DataFrame(all_forecasts)\n",
        "print(\"\\n Forecast Results Sample\")\n",
        "print(forecast_result.head())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r-KJy5jzLWdn",
        "outputId": "cc56bdfa-b52a-4a0d-fca3-2609b1f8efb1"
      },
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Forecast Weeks: DatetimeIndex(['2024-09-07', '2024-09-14', '2024-09-21', '2024-09-28',\n",
            "               '2024-10-05', '2024-10-12', '2024-10-19', '2024-10-26',\n",
            "               '2024-11-02', '2024-11-09', '2024-11-16', '2024-11-23',\n",
            "               '2024-11-30'],\n",
            "              dtype='datetime64[ns]', freq='W-SAT')\n",
            "\n",
            "Generating forecast for SerialNum 1.0\n",
            "\n",
            "Generating forecast for SerialNum 2.0\n",
            "\n",
            "Generating forecast for SerialNum 3.0\n",
            "\n",
            "Generating forecast for SerialNum 4.0\n",
            "\n",
            "Generating forecast for SerialNum 5.0\n",
            "\n",
            " Forecast Results Sample\n",
            "   SerailNum weekend_date  forecast_quantity\n",
            "0        1.0   2024-09-07          55.541859\n",
            "1        1.0   2024-09-14          41.585884\n",
            "2        1.0   2024-09-21          47.812176\n",
            "3        1.0   2024-09-28          51.837296\n",
            "4        1.0   2024-10-05          43.341766\n"
          ]
        }
      ]
    }
  ]
}